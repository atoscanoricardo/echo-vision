| Nombre     | Descripción                                                | Empresa     | Basado en                 | Año  | Parámetros    |
|------------|------------------------------------------------------------|-------------|---------------------------|------|---------------|
| GPT-4      | Generative Pre-trained Transformer 4                       | OpenAI      | Arquitectura Transformer  | 2022 | >175 mil millones (estimado) |
| GPT-3      | Generative Pre-trained Transformer 3                       | OpenAI      | Arquitectura Transformer  | 2020 | 175 mil millones |
| BERT       | Bidirectional Encoder Representations from Transformers   | Google      | Arquitectura Transformer  | 2018 | 110 millones (BERT-Large) |
| RoBERTa    | Robustly optimized BERT approach                           | Facebook AI | BERT (Google)             | 2019 | 355 millones (RoBERTa-Large) |
| T5         | Text-to-Text Transfer Transformer                          | Google      | Arquitectura Transformer  | 2019 | 11 mil millones (T5-11B) |
| ALBERT     | A Lite BERT                                                | Google      | BERT (Google)             | 2019 | 18 millones (ALBERT-xxlarge) |
| DistilBERT | Distilled version of BERT                                  | Hugging Face| BERT (Google)             | 2019 | 66 millones |
| XLNet      | Generalized Autoregressive Pretraining for Language Understanding | Google/CMU | Arquitectura Transformer y entrenamiento autoregresivo | 2019 | 340 millones (XLNet-Large) |
| ELECTRA    | Efficiently Learning an Encoder that Classifies Token Replacements Accurately | Google | Arquitectura Transformer  | 2020 | 330 millones (ELECTRA-Large) |
| ERNIE      | Enhanced Representation through kNowledge IntEgration      | Baidu       | BERT (Google)             | 2019 | 340 millones (ERNIE 2.0 Large) |